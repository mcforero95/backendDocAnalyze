
# üìÑ Backend y Frontend de An√°lisis de Documentos con LLM

Proyecto para an√°lisis, resumen y consulta de documentos utilizando FastAPI, recuperaci√≥n de contexto (RAG), Gemini IA y PostgreSQL.

---

## üöÄ Tecnolog√≠as utilizadas

- **Backend:** Python 3.10, FastAPI, PostgreSQL, Alembic(Para Migraci√≥n BD)
- **LLM:** Gemini IA de Google v√≠a API Flash 2.0
- **Embeddings locales:** Sentence Transformers (`all-MiniLM-L6-v2`)
- **Orquestaci√≥n:** Docker y Docker Compose
- **Despliegue:** GCP

---

## üõ†Ô∏è Instalaci√≥n y ejecuci√≥n local

### ‚úÖ Requisitos previos

- Docker y Docker Compose
- Clave de API de Google para Gemini IA
- Variables de entorno configuradas
- IP VM de la BD.

### ‚úÖ Variables de entorno

```env
# Base de datos PostgreSQL 
DATABASE_URL=postgresql://postgres:analyzedb@<IP VM>:5432/analyze_db
POSTGRES_DB=analyze_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=analyzedb
POSTGRES_HOST=<IP VM>
POSTGRES_PORT=5432

# Ruta de los archivos compartidos nfs
SHARED_FILES_PATH=/mnt/nfs_shared

# Seguridad JWT
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Ruta del modelo de lenguaje`
GEMINI_API_KEY=<Aqui tu apikey>
GEMINI_MODEL_NAME=gemini-2.0-flash

# Redis (para Celery)
REDIS_URL=redis://redis:6379/0
```

---

## üß† Funcionalidades clave

- ‚úÖ Subida de documentos (`.pdf`, `.docx`, `.txt`)
- ‚úÖ Extracci√≥n y segmentaci√≥n del texto en *chunks*
- ‚úÖ Generaci√≥n de **embeddings** por chunk
- ‚úÖ Resumen del documento usando **Gemini**
- ‚úÖ Chat con el documento v√≠a preguntas/respuestas
- ‚úÖ **Recuperaci√≥n de Contexto (RAG)** con chunks relevantes
- ‚úÖ Historial de conversaciones y mensajes por usuario

---

## üìå C√≥mo usar RAG para preguntas al LLM sobre el documento

### Endpoint:
`POST /ask/{document_id}?rag=true`

### Params:
```
| rag       | true                 |
| question  | Pregunta del usuario |
```
---

## üß± Modelo de datos principal
```
+------------------+-------------------+---------------------------+----+----+---------------------+
| Tabla            | Columna           | Tipo                      | PK | FK | Referencia          |
+------------------+-------------------+---------------------------+----+----+---------------------+
| alembic_version  | version_num       | character varying         | Y  | N  | ‚Äî                   |
+------------------+-------------------+---------------------------+----+----+---------------------+
| analysis         | id                | integer                   | Y  | N  | ‚Äî                   |
| analysis         | document_id       | integer                   | N  | Y  | documents(id)       |
| analysis         | result            | text                      | N  | N  | ‚Äî                   |
| analysis         | summary           | text                      | N  | N  | ‚Äî                   |
+------------------+-------------------+---------------------------+----+----+---------------------+
| conversations    | id                | integer                   | Y  | N  | ‚Äî                   |
| conversations    | user_id           | integer                   | N  | Y  | users(id)           |
| conversations    | document_id       | integer                   | N  | Y  | documents(id)       |
| conversations    | created_at        | timestamp without tz      | N  | N  | ‚Äî                   |
+------------------+-------------------+---------------------------+----+----+---------------------+
| document_chunks  | id                | integer                   | Y  | N  | ‚Äî                   |
| document_chunks  | document_id       | integer                   | N  | Y  | documents(id)       |
| document_chunks  | chunk_text        | text                      | N  | N  | ‚Äî                   |
| document_chunks  | embedding         | json                      | N  | N  | ‚Äî                   |
| document_chunks  | chunk_index       | integer                   | N  | N  | ‚Äî                   |
+------------------+-------------------+---------------------------+----+----+---------------------+
| documents        | id                | integer                   | Y  | N  | ‚Äî                   |
| documents        | title             | character varying         | N  | N  | ‚Äî                   |
| documents        | content           | character varying         | N  | N  | ‚Äî                   |
| documents        | owner_id          | integer                   | N  | Y  | users(id)           |
+------------------+-------------------+---------------------------+----+----+---------------------+
| messages         | id                | integer                   | Y  | N  | ‚Äî                   |
| messages         | conversation_id   | integer                   | N  | Y  | conversations(id)   |
| messages         | question          | text                      | N  | N  | ‚Äî                   |
| messages         | answer            | text                      | N  | N  | ‚Äî                   |
| messages         | created_at        | timestamp without tz      | N  | N  | ‚Äî                   |
+------------------+-------------------+---------------------------+----+----+---------------------+
| users            | id                | integer                   | Y  | N  | ‚Äî                   |
| users            | username          | character varying         | N  | N  | ‚Äî                   |
| users            | email             | character varying         | N  | N  | ‚Äî                   |
| users            | hashed_password   | character varying         | N  | N  | ‚Äî                   |
+------------------+-------------------+---------------------------+----+----+---------------------+
```

### Relaciones:
- analysis.document_id        ‚Üí documents.id
- conversations.user_id       ‚Üí users.id
- conversations.document_id   ‚Üí documents.id
- document_chunks.document_id ‚Üí documents.id
- documents.owner_id          ‚Üí users.id
- messages.conversation_id    ‚Üí conversations.id

---

### ‚úÖ Construcci√≥n y ejecuci√≥n

Desde la carpeta del backend (`backendDocAnalyze/`):

#### üîπ Construir los contenedores:

Para iniciar en una VM GCP: 

```bash
docker-compose -f docker-compose.worker.yml build --no-cache
```
Para iniciar local:

```bash
docker-compose -f docker-compose.yml build --no-cache
```

#### üîπ Levantar los contenedores:

Para levantar en una VM GCP:

```bash
docker-compose -f docker-compose.worker.yml up -d
```

Para levantar en local:

```bash
docker-compose -f docker-compose.yml up -d
```

#### üîπ Ver logs en tiempo real del backend:
```bash
docker logs -f backend_doc_analyze
```

#### üîπ Detener todos los contenedores:
```bash
docker-compose down
```

#### üîπ Limpiar contenedores e im√°genes:
```bash
docker system prune -a
```


### Luejo de levantar contenedores ejecuta la migraci√≥n de la BD con Alembic (PRIMER USO).

#### Solo para primer uso o instalaci√≥n:

1. Accede al backend:
    ```bash
    docker exec -it backend_doc_analyze bash
    ```

2. Crea la carpeta de migraciones (si no existe):
    ```bash
    mkdir -p /app/alembic/versions
    ```

3. Genera la migraci√≥n inicial:
    ```bash
    alembic revision --autogenerate -m "Initial migration"
    alembic upgrade head
    ```
4. Solo si hay actualizaciones sobre el modelo de datos ejecuta el siguiente comando:
    ```bash
    alembic upgrade head
    ```

## üåê Accesos

- **Frontend:** http://35.209.28.34/
- **Backend (FastAPI docs):** http://35.209.132.4:8000/docs