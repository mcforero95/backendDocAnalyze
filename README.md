# üìÑ Backend y Frontend de An√°lisis de Documentos con LLM

Proyecto para an√°lisis, resumen y consulta de documentos utilizando FastAPI, recuperaci√≥n de contexto (RAG), Gemini IA y PostgreSQL, desplegado sobre GCP de forma escalable.

---

## üöÄ Tecnolog√≠as utilizadas

- **Backend:** Python 3.10, FastAPI, PostgreSQL (Cloud SQL), Alembic
- **LLM:** Gemini IA de Google v√≠a API Flash 2.0
- **Embeddings locales:** Sentence Transformers (`all-MiniLM-L6-v2`)
- **Mensajer√≠a:** Google Cloud Pub/Sub
- **Almacenamiento de documentos:** Google Cloud Storage
- **Orquestaci√≥n:** Docker
- **Despliegue:** GCP (Compute Engine + Load Balancer + Autoscaling)

---

## üõ†Ô∏è Instalaci√≥n y ejecuci√≥n local

### ‚úÖ Requisitos previos

- Docker y Docker Compose
- Clave de API de Google para Gemini IA
- Variables de entorno configuradas
- IP p√∫blica de tu instancia de Cloud SQL (Base de Datos PostgreSQL en GCP).

### ‚úÖ Variables de entorno

```env
# Base de datos PostgreSQL (Cloud SQL)
DATABASE_URL=postgresql://postgres:analyzedb@<IP CLOUD SQL>:5432/analyze_db
POSTGRES_DB=analyze_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=analyzedb
POSTGRES_HOST=<IP CLOUD SQL>
POSTGRES_PORT=5432

# Seguridad JWT
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Ruta del modelo de lenguaje
GEMINI_API_KEY=<Tu API Key de Gemini>
GEMINI_MODEL_NAME=gemini-2.0-flash

# Redis (para cachear datos)
REDIS_URL=redis://redis:6379/0

# Configuraci√≥n de GCP
GCP_PROJECT_ID=<ID Proyecto GCP>
CLOUD_STORAGE_BUCKET=doc-analyze-uploads
PUBSUB_TOPIC=doc-process-topic
PUBSUB_SUBSCRIPTION=doc-process-subscription
```

---

## üß† Funcionalidades clave

- ‚úÖ Subida de documentos (`.pdf`, `.docx`, `.txt`) a **Cloud Storage**.
- ‚úÖ Procesamiento de documentos de forma as√≠ncrona usando **Pub/Sub** y **Workers**.
- ‚úÖ Extracci√≥n y segmentaci√≥n del texto en *chunks*.
- ‚úÖ Generaci√≥n de **embeddings** por chunk.
- ‚úÖ Resumen autom√°tico del documento usando **Gemini IA**.
- ‚úÖ Chat de preguntas y respuestas sobre el contenido.
- ‚úÖ **Recuperaci√≥n de contexto (RAG)** usando los chunks relevantes.
- ‚úÖ Historial de conversaciones por usuario.
- ‚úÖ Escalabilidad autom√°tica en GCP con **Autoscaling** y **Load Balancer**.

---

## üìå C√≥mo usar RAG para preguntas al LLM sobre el documento

### Endpoint:
`POST /ask/{document_id}?rag=true`

### Params:
```
| rag       | true                 |
| question  | Pregunta del usuario |
```

---
```
---

## üß± Modelo de datos principal
```
+------------------+-------------------+---------------------------+----+----+---------------------+
| Tabla            | Columna           | Tipo                      | PK | FK | Referencia          |
+------------------+-------------------+---------------------------+----+----+---------------------+
| alembic_version  | version_num       | character varying         | Y  | N  | ‚Äî                   |
+------------------+-------------------+---------------------------+----+----+---------------------+
| analysis         | id                | integer                   | Y  | N  | ‚Äî                   |
| analysis         | document_id       | integer                   | N  | Y  | documents(id)       |
| analysis         | result            | text                      | N  | N  | ‚Äî                   |
| analysis         | summary           | text                      | N  | N  | ‚Äî                   |
+------------------+-------------------+---------------------------+----+----+---------------------+
| conversations    | id                | integer                   | Y  | N  | ‚Äî                   |
| conversations    | user_id           | integer                   | N  | Y  | users(id)           |
| conversations    | document_id       | integer                   | N  | Y  | documents(id)       |
| conversations    | created_at        | timestamp without tz      | N  | N  | ‚Äî                   |
+------------------+-------------------+---------------------------+----+----+---------------------+
| document_chunks  | id                | integer                   | Y  | N  | ‚Äî                   |
| document_chunks  | document_id       | integer                   | N  | Y  | documents(id)       |
| document_chunks  | chunk_text        | text                      | N  | N  | ‚Äî                   |
| document_chunks  | embedding         | json                      | N  | N  | ‚Äî                   |
| document_chunks  | chunk_index       | integer                   | N  | N  | ‚Äî                   |
+------------------+-------------------+---------------------------+----+----+---------------------+
| documents        | id                | integer                   | Y  | N  | ‚Äî                   |
| documents        | title             | character varying         | N  | N  | ‚Äî                   |
| documents        | content           | character varying         | N  | N  | ‚Äî                   |
| documents        | owner_id          | integer                   | N  | Y  | users(id)           |
+------------------+-------------------+---------------------------+----+----+---------------------+
| messages         | id                | integer                   | Y  | N  | ‚Äî                   |
| messages         | conversation_id   | integer                   | N  | Y  | conversations(id)   |
| messages         | question          | text                      | N  | N  | ‚Äî                   |
| messages         | answer            | text                      | N  | N  | ‚Äî                   |
| messages         | created_at        | timestamp without tz      | N  | N  | ‚Äî                   |
+------------------+-------------------+---------------------------+----+----+---------------------+
| users            | id                | integer                   | Y  | N  | ‚Äî                   |
| users            | username          | character varying         | N  | N  | ‚Äî                   |
| users            | email             | character varying         | N  | N  | ‚Äî                   |
| users            | hashed_password   | character varying         | N  | N  | ‚Äî                   |
+------------------+-------------------+---------------------------+----+----+---------------------+
```

### Relaciones:
- analysis.document_id        ‚Üí documents.id
- conversations.user_id       ‚Üí users.id
- conversations.document_id   ‚Üí documents.id
- document_chunks.document_id ‚Üí documents.id
- documents.owner_id          ‚Üí users.id
- messages.conversation_id    ‚Üí conversations.id

---

## ‚úÖ Construcci√≥n y ejecuci√≥n

Desde la carpeta del backend (`backendDocAnalyze/`):

#### üîπ Construir contenedores:

```bash
docker-compose build --no-cache
```

#### üîπ Levantar contenedores:

```bash
docker-compose up -d
```

#### üîπ Ver logs del backend:

```bash
docker logs -f backend_doc_analyze
```

#### üîπ Detener todos los contenedores:

```bash
docker-compose down
```

#### üîπ Limpiar contenedores e im√°genes:

```bash
docker system prune -a
```

---

### üóÑÔ∏è Migraciones de base de datos (Primer uso o cambios de modelos)

1. Accede al contenedor backend:
    ```bash
    docker exec -it backend_doc_analyze bash
    ```

2. Crea carpeta de migraciones (si no existe):
    ```bash
    mkdir -p /app/alembic/versions
    ```

3. Genera la migraci√≥n inicial:
    ```bash
    alembic revision --autogenerate -m "Initial migration"
    alembic upgrade head
    ```

4. Posteriormente, solo ejecutar cuando haya cambios en el modelo de datos:
    ```bash
    alembic upgrade head
    ```

---

## üåê Accesos

- **Frontend:** http://<IP Frontend>/
- **Backend (FastAPI docs):** http://<IP Backend>:8000/docs

---

# ‚ö°Ô∏è Notas importantes

- Los archivos **no se almacenan en disco local**. Todo se guarda en **Cloud Storage**.
- El procesamiento de documentos es **as√≠ncrono** y distribuido entre m√∫ltiples **Workers**.
- La base de datos ahora es **Cloud SQL** en GCP, no local.
- El sistema es escalable a cientos o miles de usuarios gracias a **Autoscaling** de GCP.

---
