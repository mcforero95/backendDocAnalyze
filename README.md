# üìÑ Backend de An√°lisis de Documentos con LLM

Proyecto de an√°lisis, resumen y consulta de documentos usando FastAPI y modelos LLM locales.

## üöÄ Tecnolog√≠as

- Python 3.10
- FastAPI
- PostgreSQL
- Redis
- Llama.cpp
- Docker y Docker Compose

## üõ†Ô∏è Instalaci√≥n y Ejecuci√≥n

### Requisitos
- Docker
- Docker Compose
- Modelo LLM en formato `.gguf`

### Configuraci√≥n
Crea un archivo `.env` con las siguientes variables:

```env
# Base de datos PostgreSQL
DATABASE_URL=postgresql://postgres:<CONTRASE√ëA>@<IP_CLOUD_SQL>:5432/analyze_db
POSTGRES_DB=analyze_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=<CONTRASE√ëA>
POSTGRES_HOST=<IP_CLOUD_SQL>
POSTGRES_PORT=5432

# Redis podr√≠a quedar igual si es local
REDIS_URL=redis://redis:6379/0


# Seguridad JWT
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Ruta del modelo LLM local
LLM_MODEL_PATH=./models/llama.gguf

# Redis (para Celery)
REDIS_URL=redis://redis:6379/0
